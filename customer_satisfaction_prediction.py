# -*- coding: utf-8 -*-
"""Customer_satisfaction_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i5-GHyWoqMm7INjvsPm76bzHzZKPiTCg
"""

import os
import json
import joblib
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score
from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer
from sklearn.impute import SimpleImputer
from sklearn.metrics import (
    accuracy_score, f1_score, classification_report, confusion_matrix,
    roc_curve, auc, precision_recall_curve
)
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import TruncatedSVD
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import hstack

import lightgbm as lgb

warnings.filterwarnings("ignore")

# ----------------------------
# Utility Functions
# ----------------------------
def quadratic_weighted_kappa(y_true, y_pred, min_rating=1, max_rating=5):
    y_true, y_pred = np.asarray(y_true,int), np.asarray(y_pred,int)
    num_ratings = max_rating - min_rating + 1
    conf_mat = np.zeros((num_ratings,num_ratings))
    for a,b in zip(y_true,y_pred):
        conf_mat[a-min_rating,b-min_rating]+=1
    hist_true, hist_pred = conf_mat.sum(1), conf_mat.sum(0)
    expected = np.outer(hist_true,hist_pred)/max(1,conf_mat.sum())
    W = np.zeros((num_ratings,num_ratings))
    for i in range(num_ratings):
        for j in range(num_ratings):
            W[i,j] = ((i-j)**2)/((num_ratings-1)**2)
    return 1.0 - (np.sum(W*conf_mat)/max(1e-12,np.sum(W*expected)))

def parse_datetime_safe(s):
    return pd.to_datetime(s, errors="coerce", utc=True)

def duration_hours(start,end):
    return (parse_datetime_safe(end)-parse_datetime_safe(start)).dt.total_seconds()/3600

def combine_text_cols(df,cols):
    return df[cols].fillna("").agg(" ".join,axis=1) if set(cols)&set(df.columns) else pd.Series([""]*len(df),index=df.index)

KEYWORDS = ["refund","return","not working","broken","cancel","delay","late","replace","warranty"]

def text_stats_df(df, text_cols):
    out = pd.DataFrame(index=df.index)
    for c in text_cols:
        s = df.get(c,pd.Series("",index=df.index)).fillna("").astype(str)
        out[f"{c}_char_len"] = s.str.len()
        out[f"{c}_word_len"] = s.str.split().apply(len)
        out[f"{c}_caps_ratio"] = s.apply(lambda x: sum(ch.isupper() for ch in x)/max(1,len(x)))
        out[f"{c}_excl_count"] = s.str.count("!")
        out[f"{c}_q_count"] = s.str.count(r"\?")
        for kw in KEYWORDS:
            out[f"{c}_kw_{kw.replace(' ','_')}"] = s.str.lower().str.contains(kw).astype(int)
    return out

def date_features(df):
    out = pd.DataFrame(index=df.index)
    if "Date of Purchase" in df.columns:
        dop = parse_datetime_safe(df["Date of Purchase"])
        out["days_since_purchase"] = (pd.Timestamp.utcnow(tz="UTC")-dop).dt.total_seconds()/86400
        out["days_since_purchase"] = out["days_since_purchase"].clip(upper=365*5)
    if {"Date of Purchase","First Response Time"} <= set(df.columns):
        out["hrs_purchase_to_first_response"] = duration_hours(df["Date of Purchase"],df["First Response Time"])
    if {"First Response Time","Time to Resolution"} <= set(df.columns):
        out["hrs_first_response_to_resolution"] = duration_hours(df["First Response Time"],df["Time to Resolution"])
    return out

# Define eng_features at the global scope
def eng_features(X, text_cols):
    txt = text_stats_df(X,[c for c in text_cols if c in X.columns])
    dates = date_features(X)
    prio = X.get("Ticket Priority",pd.Series(index=X.index)).map({"Low":0,"Medium":1,"High":2,"Critical":3}).to_frame("priority_ord")
    return pd.concat([txt,dates,prio],axis=1)

# Define fit_vec and transform_vec at the global scope
word_v_global = TfidfVectorizer(stop_words="english",ngram_range=(1,2),min_df=3,max_features=30_000)
char_v_global = TfidfVectorizer(analyzer="char",ngram_range=(3,5),min_df=3,max_features=20_000)
def fit_vec(X, text_cols):
    text = combine_text_cols(X,text_cols)
    word_v_global.fit(text)
    char_v_global.fit(text)
    return np.zeros((len(X),0))

def transform_vec(X, text_cols, use_svd=True):
    text = combine_text_cols(X,text_cols)
    tfidf = hstack([word_v_global.transform(text), char_v_global.transform(text)])
    if use_svd:
        svd = TruncatedSVD(n_components=200, random_state=42)
        return svd.fit_transform(tfidf)
    return tfidf


# ----------------------------
# Preprocessor
# ----------------------------
def build_preprocessor(text_cols, cat_cols=None, use_svd=True):
    if cat_cols is None:
        cat_cols = ["Customer Gender","Product Purchased","Ticket Type","Ticket Status","Ticket Channel"]

    numeric_transformer = Pipeline([
        ("imp",SimpleImputer(strategy="median")),
        ("sc",StandardScaler(with_mean=False))
    ])

    categorical_transformer = Pipeline([
        ("imp",SimpleImputer(strategy="most_frequent")),
        ("ohe",OneHotEncoder(handle_unknown="ignore", sparse_output=True, min_frequency=0.005))
    ])

    prep = ColumnTransformer([
        ("eng",Pipeline([("fe",FunctionTransformer(eng_features,validate=False, kw_args={"text_cols": text_cols})),("num",numeric_transformer)]),[]),
        ("cat",categorical_transformer,cat_cols),
        ("txt",FunctionTransformer(transform_vec,validate=False, kw_args={"text_cols": text_cols, "use_svd": use_svd}),text_cols)
    ],sparse_threshold=0.3)

    # Remove local fit_vec and transform_vec attributes
    # prep.word_v, prep.char_v, prep._fit_text = word_v_global, char_v_global, fit_vec
    return prep

# ----------------------------
# Model Training
# ----------------------------
def train_and_eval(df, target_col="Customer Satisfaction Rating", text_cols=["Ticket Subject","Ticket Description","Resolution"], test_size=0.2, model_type="logreg"):
    df = df.drop(columns=["Ticket ID","Customer Name","Customer Email"],errors="ignore")
    df = df[df[target_col].notnull()].copy()
    df[target_col] = df[target_col].astype(float).round().astype(int)

    y, X = df[target_col], df.drop(columns=[target_col])
    prep = build_preprocessor(text_cols)
    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=test_size,stratify=y,random_state=42)
    fit_vec(X_train, text_cols) # Use the global fit_vec

    if model_type == "logreg":
        clf = LogisticRegression(multi_class="multinomial",solver="saga",max_iter=10000,class_weight="balanced")
    elif model_type == "lightgbm":
        clf = lgb.LGBMClassifier(objective="multiclass", num_class=len(np.unique(y)), n_estimators=300, learning_rate=0.05, random_state=42)
    else:
        raise ValueError("Unsupported model_type")

    pipe = Pipeline([("prep",prep),("clf",clf)])

    cv = StratifiedKFold(3,shuffle=True,random_state=42)
    cv_scores = cross_val_score(pipe, X_train, y_train, cv=cv, scoring="accuracy")

    pipe.fit(X_train,y_train)
    y_pred = pipe.predict(X_test)

    metrics = {
        "model": model_type,
        "cv_accuracy_mean": float(np.mean(cv_scores)),
        "test_accuracy": accuracy_score(y_test,y_pred),
        "macro_f1": f1_score(y_test,y_pred,average="macro"),
        "qwk": quadratic_weighted_kappa(y_test,y_pred),
        "report": classification_report(y_test,y_pred,output_dict=True),
        "confusion_matrix": confusion_matrix(y_test,y_pred).tolist(),
        "y_test": y_test,
        "y_pred": y_pred,
        "pipe": pipe
    }
    return pipe, metrics

# ----------------------------
# Visualization
# ----------------------------
def plot_confusion_matrix(y_true,y_pred):
    cm = confusion_matrix(y_true,y_pred)
    plt.figure(figsize=(6,5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.title("Confusion Matrix")
    plt.show()

def plot_model_comparison(results):
    df = pd.DataFrame(results)
    metrics_to_plot = ["test_accuracy","macro_f1","qwk"]

    plt.figure(figsize=(8,5))
    for i, metric in enumerate(metrics_to_plot):
        plt.bar([r+0.2*i for r in range(len(df))], df[metric], width=0.2, label=metric)
    plt.xticks([r+0.2 for r in range(len(df))], df["model"])
    plt.ylabel("Score")
    plt.title("Model Performance Comparison")
    plt.legend()
    plt.show()

def plot_precision_recall_roc(metrics):
    y_true = metrics["y_test"]
    y_pred = metrics["y_pred"]
    classes = np.unique(y_true)

    # Precision-Recall curves
    plt.figure(figsize=(10,5))
    for cls in classes:
        precision, recall, _ = precision_recall_curve((y_true==cls).astype(int), (y_pred==cls).astype(int))
        plt.plot(recall, precision, label=f"Class {cls}")
    plt.xlabel("Recall")
    plt.ylabel("Precision")
    plt.title("Precision-Recall Curves")
    plt.legend()
    plt.show()

    # ROC curves
    plt.figure(figsize=(10,5))
    for cls in classes:
        fpr, tpr, _ = roc_curve((y_true==cls).astype(int), (y_pred==cls).astype(int))
        plt.plot(fpr, tpr, label=f"Class {cls} (AUC = {auc(fpr,tpr):.2f})")
    plt.plot([0,1],[0,1],'k--')
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC Curves")
    plt.legend()
    plt.show()

# ----------------------------
# Example Usage (for Jupyter/Colab)
# ----------------------------
if __name__ == "__main__":
    # load dataset safely (try multiple paths)
    file_candidates = ["customer_support_tickets.csv", "./data/customer_support_tickets.csv", "/data/customer_support_tickets.csv"]
    csv_path = next((f for f in file_candidates if os.path.exists(f)), None)
    if csv_path is None:
        raise FileNotFoundError("customer_support_tickets.csv not found in working directory or data folders.")

    df = pd.read_csv(csv_path)

    # Logistic Regression
    logreg_model, logreg_metrics = train_and_eval(df, model_type="logreg")
    print("Logistic Regression Metrics:\\n", json.dumps({k:v for k,v in logreg_metrics.items() if k not in ["y_test","y_pred","pipe"]}, indent=2))

    # LightGBM
    lgbm_model, lgbm_metrics = train_and_eval(df, model_type="lightgbm")
    print("LightGBM Metrics:\\n", json.dumps({k:v for k,v in lgbm_metrics.items() if k not in ["y_test","y_pred","pipe"]}, indent=2))

    # Comparison plot
    plot_model_comparison([logreg_metrics, lgbm_metrics])

    # Precision-Recall & ROC curves for LightGBM
    plot_precision_recall_roc(lgbm_metrics)

    # Save best model
    joblib.dump(lgbm_model, "customer_support_model.pkl")

